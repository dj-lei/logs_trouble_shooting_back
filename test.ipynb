{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbc2e3a-893f-4652-b085-d5bb4c10dfc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import zlib\n",
    "import base64\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import configparser\n",
    "from utils import *\n",
    "from sys import platform\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "import torch\n",
    "from tslearn.metrics import dtw, dtw_path\n",
    "from tslearn.metrics import lcss, lcss_path\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-2_H-128_A-2')\n",
    "model = BertModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "model = model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0253b509-184b-441e-9a6c-08d7d625752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = configparser.ConfigParser()\n",
    "cf.read('config/config.cfg')\n",
    "\n",
    "env = 'DEVELOP'\n",
    "if 'win' in platform:\n",
    "    env = 'DEVELOP'\n",
    "elif 'linux' in platform:\n",
    "    env = 'PRODUCT'\n",
    "    \n",
    "class EsCtrl(object):\n",
    "    def __init__(self):\n",
    "        self.es_ctrl = Elasticsearch(cf['ENV_'+env]['ADDR'], ca_certs=cf['ELASTICSEARCH']['CA_CERTS'])\n",
    "\n",
    "    def query_index_logs(self, index):\n",
    "        # query = {\n",
    "        #     \"match\": {\n",
    "        #         \"trace\": \"com_ericsson_trithread:INFO\"\n",
    "        #     }\n",
    "        # }\n",
    "        #data = self.es_ctrl.search(index=index, query=query, scroll='1s', size=10000)\n",
    "        data = self.es_ctrl.search(index=index, scroll='1s', size=10000)\n",
    "        sid = data['_scroll_id']\n",
    "        scroll_size = len(data['hits']['hits'])\n",
    "        res = []\n",
    "        while scroll_size > 0:\n",
    "            # Before scroll, process current batch of hits\n",
    "            res.extend(data['hits']['hits'])\n",
    "            data = self.es_ctrl.scroll(scroll_id=sid, scroll='1s')\n",
    "            # Update the scroll ID\n",
    "            sid = data['_scroll_id']\n",
    "            # Get the number of results that returned in the last scroll\n",
    "            scroll_size = len(data['hits']['hits'])\n",
    "        return res\n",
    "\n",
    "    def query_indices(self):\n",
    "        res = []\n",
    "        for key in self.es_ctrl.indices.get_alias().keys():\n",
    "            if len(key) > 0:\n",
    "                if '.analyzed_' in key:\n",
    "                    res.append(key.replace('.analyzed_', ''))\n",
    "        return res\n",
    "\n",
    "    def is_exists(self, index):\n",
    "        return self.es_ctrl.indices.exists(index=index)\n",
    "\n",
    "    def count_index(self, index):\n",
    "        return self.es_ctrl.count(index=index)['count']\n",
    "\n",
    "    def store_index(self, index, data):\n",
    "        data = deflate_and_base64_encode(json.dumps(data).encode('utf-8'))\n",
    "        return self.es_ctrl.index(index=index, body={'content': data})\n",
    "\n",
    "    def query_index(self, index):\n",
    "        data = self.es_ctrl.search(index=index)\n",
    "        data = json.loads(decode_base64_and_inflate(data['hits']['hits'][0]['_source']['content']))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f2c72c-6dd7-4e5f-b00a-861983fc0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Data Clean ################################################\n",
    "def package_kv(df):\n",
    "    res = {}\n",
    "    k_type = {}\n",
    "    for i, (kv,index,timestamp) in enumerate(zip(df.kv.values, df['index'].values, df.timestamp.values)):\n",
    "        for item in kv:\n",
    "            if len(item[1]) > 0:\n",
    "                if item[0] in res:\n",
    "                    res[item[0]].append(item[1]  + [timestamp] + [str(i)] + [index]) # [value1,value2,value3,timestamp,process_index,global_index]\n",
    "                else:\n",
    "                    res[item[0]] = [item[1]  + [timestamp] + [str(i)] + [index]]\n",
    "    for key in res.keys():\n",
    "        width = max(map(len, res[key])) # get max width\n",
    "        type_list = []\n",
    "        for i, item in enumerate(res[key]):\n",
    "            if '0x' in item[0]:\n",
    "                type_list.append('register')\n",
    "            elif item[0].isupper():\n",
    "                type_list.append('discrete')\n",
    "            else:\n",
    "                type_list.append('continuous')\n",
    "                \n",
    "            if len(item) != width:\n",
    "                tmp = [0 for _ in range(0, width)]\n",
    "                tmp[-3] = item[-3]\n",
    "                tmp[-2] = item[-2]\n",
    "                tmp[-1] = item[-1]\n",
    "                res[key][i] = tmp\n",
    "        res[key] = np.array(res[key]).transpose().tolist() # matrix transposition\n",
    "        k_type[key] = 'discrete' if len(set(type_list)) > 1 else list(set(type_list))[0]\n",
    "    return res,k_type\n",
    "\n",
    "\n",
    "def package_inverted_index_table(table, key, data):\n",
    "    def clean_special_symbols(text):\n",
    "        for ch in ['/','*','{','}','[',']','(',')','#','+','-','!','=',':',',','\"']:\n",
    "            if ch in text:\n",
    "                text = text.replace(ch,\" \")\n",
    "        return re.sub(\" +\", \" \", text)\n",
    "    for index, msg in data:\n",
    "        for word in set(clean_special_symbols(msg).split(' ')):\n",
    "            w = word.lower()\n",
    "            if w not in table:\n",
    "                table[w] = {'x': [index], 'y': [key]} # x:global index, y: yaxis num \n",
    "            else:\n",
    "                table[w]['x'].append(index)\n",
    "                table[w]['y'].append(key)\n",
    "\n",
    "\n",
    "def clean_data(esdata):\n",
    "    def clean_msg_special_symbols(text):\n",
    "        for ch in ['{', '}', '[', ']', '(', ')', '\"', '::']:\n",
    "            if ch in text:\n",
    "                text = text.replace(ch, \" \")\n",
    "        return re.sub(\" +\", \" \", text)\n",
    "\n",
    "    story = []\n",
    "    for item in esdata:\n",
    "        if 'msg' in item['_source']:\n",
    "            tmp = clean_msg_special_symbols(item['_source']['msg'])\n",
    "            if len(re.findall('process \\= (.*?)$', tmp)) > 0:\n",
    "                process = re.findall('process \\= (.*?),', tmp)[0]\n",
    "                msg = re.findall('msg \\= (.*?)$', tmp)[0]\n",
    "            #                 fileAndLine = re.findall('fileAndLine \\= \\\"(.*?)\\\"',item['_source']['msg'])[0].split(':')[0]\n",
    "            # elif len(re.findall('procname \\= (.*?)$', tmp)) > 0:\n",
    "            #     process = re.findall('procname \\= (.*?),', tmp)[0]\n",
    "            #     #                 msg = tmp.split(',')[2].replace('\"','').replace('}','').replace('{','')\n",
    "            #     msg = tmp\n",
    "            else:\n",
    "                process = 'main'\n",
    "                msg = tmp\n",
    "                \n",
    "            msg = msg.replace('= ',':').replace(' = ',':').replace(': ',':').replace(' : ',':').replace('=',':')\n",
    "\n",
    "            for elm in re.split('[: ]',msg):\n",
    "                if elm.isupper():\n",
    "                    msg = re.sub('[: ]'+elm, ':'+elm, msg)\n",
    "\n",
    "            msg = re.sub('(:(?!-).*?[ $])', r'\\1,', (msg + ' $'))\n",
    "            # msg\n",
    "            kv = []\n",
    "            for k, v in re.findall('([A-Za-z0-9_.]+?)[ ]?[:=][ ]?(.*?)[,$]', msg):\n",
    "                if len(v.strip()) > 0:\n",
    "                    if (v.strip()+'xx').lower()[0:2] == '0x':\n",
    "                        kv.append((k.strip()+'(r)',  [v.strip()]))\n",
    "                    elif v.strip()[0].isalpha():\n",
    "                        kv.append((k.strip()+'(d)', [v.strip()]))\n",
    "                    else:\n",
    "                        kv.append((k.strip()+'(c)', re.findall('[0-9.]+', v)))\n",
    "            story.append([item['_source']['device'], item['_source']['trace'], process,  item['_source']['logtime'][:-1] + '.' + str(item['_source']['millisecond']) + 'Z', item['_source']['msg'], kv])\n",
    "\n",
    "    story = pd.DataFrame(story, columns=['device', 'trace', 'process', 'timestamp', 'msg', 'kv']).sort_values('timestamp',ascending=True).reset_index(drop=True)\n",
    "    story_line = {}\n",
    "    inverted_index_table = {}\n",
    "    for dev in set(story.device.values):\n",
    "        data = story.loc[(story['device'] == dev), :].reset_index(drop=True)\n",
    "        sub_inverted_index_table = {}\n",
    "        for i, process_name in enumerate(sorted(set(data.process.values), key=list(data.process.values).index)):\n",
    "            process = data.loc[(data['process'] == process_name), :].reset_index()\n",
    "            process['index'] = process['index'].astype(str)\n",
    "            process_start_time = process['timestamp'][0]\n",
    "            process_start_count = process['index'][0]\n",
    "            process_end_time = process['timestamp'][process.shape[0] - 1]\n",
    "            process_end_count = process['index'][process.shape[0] - 1]\n",
    "            package_inverted_index_table(sub_inverted_index_table, i, zip(process['index'].values, process.msg.values))\n",
    "            msg = dict(zip(process['index'].values, [str(a) + '||' + b + '||' + c for a, b, c in\n",
    "                                                     zip(process.index.values, process.timestamp.values,\n",
    "                                                         process.msg.values)]))\n",
    "            kv,k_type = package_kv(process)\n",
    "            if dev not in story_line:\n",
    "                story_line[dev] = [{'process': process_name, 'start_time': process_start_time, 'start_count': process_start_count, 'end_time': process_end_time, 'end_count': process_end_count, 'msg': msg, 'kv': kv}]\n",
    "            else:\n",
    "                story_line[dev].append({'process': process_name, 'start_time': process_start_time, 'start_count': process_start_count, 'end_time': process_end_time, 'end_count': process_end_count, 'msg': msg, 'kv': kv})\n",
    "        inverted_index_table[dev] = sub_inverted_index_table\n",
    "    return {'story_line': story_line, 'inverted_index_table': inverted_index_table}\n",
    "\n",
    "\n",
    "def apply_filter_by_keywords(df):\n",
    "    if (len(set(df['msg']) & set([':', '='])) > 0):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def apply_filter_digit(df):\n",
    "    return re.sub('\\d+', '', df['msg'])\n",
    "\n",
    "\n",
    "def apply_keyword_highlight(df, keywords, color_highlight):\n",
    "    tmp = [item.lower() for item in keywords]\n",
    "    for item in tmp:\n",
    "        if (item == 'abn:') & (item in df['msg'].lower()):\n",
    "            return color_highlight\n",
    "        elif len(set(df['msg'].lower().split(' ')).intersection(set(tmp))) > 0:\n",
    "            return color_highlight\n",
    "    return df['status']\n",
    "\n",
    "\n",
    "def cal_time_difference(start, end):\n",
    "    return datetime.datetime.strptime(end, \"%H:%M:%S\") - datetime.datetime.strptime(start, \"%H:%M:%S\")\n",
    "\n",
    "\n",
    "############################################ XML Compression and Decompression ################################################\n",
    "def decode_base64_and_inflate(b64string):\n",
    "    decoded_data = base64.b64decode(b64string)\n",
    "    return zlib.decompress(decoded_data , -15)\n",
    "\n",
    "\n",
    "def deflate_and_base64_encode(string_val):\n",
    "    zlibbed_str = zlib.compress(string_val)\n",
    "    compressed_string = zlibbed_str[2:-4]\n",
    "    return base64.b64encode(compressed_string).decode(\"utf-8\")\n",
    "\n",
    "############################################ Text ecoder ################################################\n",
    "def pretrained_model_encode_msg(object1, object2):\n",
    "    with torch.no_grad():\n",
    "        object1_inputs = tokenizer(list(object1.msg.values), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        object1_outputs = model(**object1_inputs)\n",
    "\n",
    "        object2_inputs = tokenizer(list(object2.msg.values), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        object2_outputs = model(**object2_inputs)\n",
    "    return object1_outputs, object2_outputs\n",
    "\n",
    "def onehot_encode_string(str1,str2):\n",
    "    data = []\n",
    "    data.extend(str1)\n",
    "    data.extend(str2)\n",
    "    values = np.array(data)\n",
    "\n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder().fit(values)\n",
    "    integer_encoded = label_encoder.transform(values)\n",
    "\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False).fit(integer_encoded)\n",
    "\n",
    "    encoder1 = onehot_encoder.transform(label_encoder.transform(str1).reshape(len(str1), 1))\n",
    "    encoder2 = onehot_encoder.transform(label_encoder.transform(str2).reshape(len(str2), 1))\n",
    "    # inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "    # print(inverted)\n",
    "    return encoder1,encoder2\n",
    "\n",
    "############################################ Space Vectors Algorithm ################################################\n",
    "def cal_lcss_path_and_score(s_y1, s_y2):\n",
    "    path, score = lcss_path(s_y1, s_y2)\n",
    "    return path, score\n",
    "\n",
    "def cal_dtw_path_and_score(s_y1, s_y2):\n",
    "    path, score = dtw_path(s_y1, s_y2)\n",
    "    return path, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e89550-8aea-4484-85a3-fdc15c89e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 'ru_lock_unlock_normal2_lab_2022-09-26-10-01-12'\n",
    "es_ctrl = EsCtrl()\n",
    "story = es_ctrl.query_index_logs(index)\n",
    "data = clean_data(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a4b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = es_ctrl.store_index('.analyzed_'+index, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['story_line']['SELF'][0]['msg']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-yesterday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae52b8-608d-4856-af18-75aec0256c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "import re\n",
    "\n",
    "path = 'GLT_SUKAMULYA_CBN_CM_220715_064452_WIB_MSRBS-GL_CXP9024418-15_R24M11_dcgm.zip'\n",
    "path_logfiles = 'GLT_SUKAMULYA_CBN_CM_logfiles.zip'\n",
    "teread = 'teread.log'\n",
    "regex = '(.*?): \\[(.*?)\\] \\((.*?)\\) (.*?) (.*?): (.*?)$'\n",
    "judge_count = 50\n",
    "logs = {}\n",
    "table =[]\n",
    "dev = ''\n",
    "log_flag = False\n",
    "is_extrac = False\n",
    "with zipfile.ZipFile(path, 'r') as outer:\n",
    "    with outer.open(path_logfiles, 'r') as nest:\n",
    "        logfiles = io.BytesIO(nest.read())\n",
    "        with zipfile.ZipFile(logfiles) as nested_zip:\n",
    "            with nested_zip.open(teread, 'r') as log:\n",
    "                lines = log.readlines()\n",
    "                count = 0\n",
    "                for i, line in enumerate(lines):\n",
    "                    line = line.decode(\"utf-8\")\n",
    "                    if len(line) > 0:\n",
    "                        if line[0] == '=':\n",
    "                            log_flag = False\n",
    "                            is_extrac = False\n",
    "                            count = 0\n",
    "                    if log_flag:\n",
    "                        logs[dev].append(line[0:-1])\n",
    "                        if count < judge_count:\n",
    "                            if (is_extrac == False):\n",
    "                                count = count + 1\n",
    "                                if len(re.findall(regex, line)) > 0:\n",
    "                                    is_extrac = True\n",
    "                        else:\n",
    "                            if (is_extrac == False):\n",
    "                                logs.pop(dev, None)\n",
    "                                log_flag = False\n",
    "                            \n",
    "                    if ('coli>' in line) & ('te log read' in line):\n",
    "                        dev = line.split(' ')[1]\n",
    "                        logs[dev] = []\n",
    "                        log_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "for key in logs.keys():\n",
    "    path = path_logfiles.split('_logfiles')[0]+'_'+key+'_'+datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    with open(path, 'w') as fp:\n",
    "        fp.write(\"\\n\".join(item for item in logs[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3caa8a-fde9-44ca-86f4-e231ebd3d26f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,elm in enumerate(data['story_line']['BXP_2051']):\n",
    "    print(i,elm['process'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e357b3d-b590-47dc-9ae9-c161218433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['story_line']['BXP_2051'][6]['k_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe7250-7976-4c1a-879b-bd4b83ad470d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['story_line']['BXP_2051'][6]['kv']['dpd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a0ecd-8513-4eee-b382-718cf750cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_msg_special_symbols(text):\n",
    "    for ch in ['{', '}', '[', ']', '(', ')', '\"', '::']:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch, \" \")\n",
    "    return re.sub(\" +\", \" \", text)\n",
    "    \n",
    "# msg = 'GaN Boost mode, set to boost mode Pma:-15.88[-41.54 -9.50] dB, DpdPma:-20.09[-20.58 -19.18] dB, Pmb:-15.88, TorPmb:-15.92[-50.88 -9.50] dB, avgTxPma:-18.14 dB, pmDpdIrqStat:0x00000000, pmScaleFactor: 65K'\n",
    "# msg = 'txAtt:145, txAttPeak:0, dpGainLoopEnable:true, dpGainCtrlType:VVA_QPB93, torTemperature:820 (0.1C), torGainBackoff:0 (0.01dB), torGainLin:3.82825(0.01dB), torStepBit:9, cc1Ctrl1=0x00000100 , avgIMpa0:690 [mAmp]'\n",
    "# msg = '[TXL_GAIN] Pma:-inf[-41.54 -9.50] dB, DpdPma:-inf[-inf -inf] dB, Pmb:-inf, TorPmb:-inf[-inf -9.50] dB, avgTxPma:-inf dB, pmDpdIrqStat:0x00000000, pmScaleFactor: 65K'\n",
    "# msg = 'New event= EVENT_DEACTIVATE carrierId= 196908 fbsId= 1 fbsState= DISABLED cycleRequired= YES 110'\n",
    "# msg = 'Event CARRIER_DEACTIVATE for carrierId:778'\n",
    "# msg = 'Set event RX_SETUP_EVENT to time: 250[ms], from 0x13000e3'\n",
    "# msg = 'New event= EVENT_SETUP carrierId= 771 fbsId= 1 fbsState= SETUP cycleRequired= NO 0'\n",
    "# msg = '0-insertion for fbsId=2 event=EVENT_RELEASE '\n",
    "msg = 'DP trace: 339: 4909172 dllb_radon.c(4780) INFO:7: Status: stat{dpd=0x00808c10 pd{0=0x00808c10, 1=0x00808410} ec=0x000002ff}'\n",
    "msg = clean_msg_special_symbols(msg)\n",
    "msg = msg.replace('= ',':').replace(' = ',':').replace(': ',':').replace(' : ',':').replace('=',':')\n",
    "\n",
    "for elm in re.split('[: ]',msg):\n",
    "    if elm.isupper():\n",
    "        msg = re.sub('[: ]'+elm, ':'+elm, msg)\n",
    "\n",
    "msg = re.sub('(:(?!-).*?[ $])', r'\\1,', (msg + ' $'))\n",
    "# msg\n",
    "kv = []\n",
    "for k, v in re.findall('([A-Za-z0-9_.]+?)[ ]?[:=][ ]?(.*?)[,$]', msg):\n",
    "    if (v.strip()+'xx').lower()[0:2] == '0x':\n",
    "        kv.append((k.strip()+'(r)',  [v.strip()]))\n",
    "    elif v.strip()[0].isalpha():\n",
    "        kv.append((k.strip()+'(d)', [v.strip()]))\n",
    "    else:\n",
    "        kv.append((k.strip()+'(c)', re.findall('[0-9.]+', v)))\n",
    "kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# msg = 'BXP_2051: [Trace log from 2 restarts before]'\n",
    "# msg = 'BXP_2050: [2022-07-14 21:47:47.132] txlProcBranchA dpdController.cc:3069 INFO:txAtt:1812, txAttPeak:0, dpGainLoopEnable:false, dpGainCtrlType:IDLE, torGainLin:2.61517, torStepBit:7, ccCtrl:0x00000100, avgIMpa0:470 [mAmp]'\n",
    "# msg = 'BXP_2051: [2022-07-09 15:53:37.007706415] (+?.?????????) radio6626 com_ericsson_trithread:INFO: { cpu_id = 3 }, { process = \"txlProcBranchE\", fileAndLine = \"dpdController.cc:2112\", msg = \"Power measurement, Pma:-14.35[-41.54 -9.50] dB, DpdPma:-18.25[-19.05 -17.65] dB, Pmb:-14.35, TorPmb:-14.42[-49.35 -9.50] dB, avgTxPma:-13.62 dB, pmDpdIrqStat:0x00008000, pmScaleFactor: 65K\" }'\n",
    "msg = '[17:46:31.176471339] (+0.000298760) air6419_mongoose com_ericsson_trithread:INFO: { cpu_id = 3 }, { process = \"txlProcBranch5\", fileAndLine = \"delayEstGen3Drv.cc:240\", msg = \"Fractional delay ok. IntegerDelay: 0x0000030c, FracDelay: 0x0000003a, dpdIrqStat: 0x04000080, dpdStat: 0x04000080, txSurveyMaxDpdAddr: 0x00000101, delEstIntFracDelta: 0xffffffff, delEstIntCorr: 0x00000000, delEstFracCorr: 0x00002000\" }'\n",
    "re.findall('(.*?): \\[(.*?)\\] \\((.*?)\\) (.*?) (.*?): (.*?)$', msg)\n",
    "# re.findall('\\[(.*?)\\] \\((.*?)\\) (.*?) (.*?): (.*?)$', msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141331db-0229-4e21-b961-689a957ae326",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc609c-38cc-4060-a1aa-dac52a02dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'0x00000100'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9ba49-26ea-4724-bd36-28751e6a4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'VVA_QPb93'.isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba88a2-a5d4-4c35-95a1-9af6f447b0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
